# AI Enablement & Automation

## Summary
Guidelines for using AI and automation to amplify scholar outcomes, reduce operational load, and protect trust.

## Goals
- Increase staff leverage without compromising privacy or fairness.
- Document approved tooling and safe usage patterns.
- Establish ownership for automation quality and drift.

## Guardrails
- Use only approved tools for sensitive data or scholar information.
- Never paste private data into consumer AI tools.
- Log AI-assisted decisions that affect eligibility or awards.
- Keep a human approval step for award or eligibility changes.

## Approved use cases
- Drafting outreach, grant updates, and cohort communications.
- Summarizing application notes and reviewer feedback.
- Preparing cohort insights from anonymized data.
- Generating first-pass FAQs and internal playbook drafts.

## Review requirements
- Human review is required before sharing externally.
- Document prompts or workflows for repeatable outputs.
- Verify facts against primary sources before publishing.
- Store final outputs in the system of record.

## Automation intake
1. Submit a short request with the workflow, time saved, and risks.
2. Operations + Data review for data handling and accuracy.
3. Pilot with a small cohort, track quality and time savings.
4. Decide to scale, revise, or retire.

## Metrics
- Hours saved per month.
- Error rate versus manual process.
- Scholar sentiment on automated touchpoints.
- Adoption across teams and functions.

## Ownership
- Operations owns the automation intake queue.
- Data & Analytics owns model evaluation and drift checks.
- Security & Privacy approves tooling and vendor updates.

## References
- Security & Privacy guide
- Data & Analytics guide
- Communications guide
