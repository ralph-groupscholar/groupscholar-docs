# Review Operations & Quality Guide

## Summary
This guide defines how Group Scholar runs application reviews with consistency,
fairness, and audit-ready quality. Use it to align reviewers, calibrate scoring,
and close the loop between program design and outcomes.

## Goals
- Deliver accurate, equitable decisions with consistent scoring.
- Reduce reviewer variance through calibration and QA checks.
- Maintain an audit-ready record of decisions and evidence.
- Improve the review experience for scholars and partners.

## Review operating model
- Review lead owns the timeline, staffing plan, and quality gates.
- Reviewers complete training, calibration, and scorecards before launch.
- QA reviewers run spot checks, variance analysis, and escalation.
- Program owner approves final award slate and exception handling.

## Pre-review readiness
- Confirm rubric, weights, and eligibility rules are final and versioned.
- Validate application data completeness and flag missing items.
- Assign reviewers by workload, expertise, and conflict-of-interest rules.
- Run a dry-run review with 5-10 sample applications.

## Calibration and scoring
- Hold calibration sessions with scored exemplars before launch.
- Provide scoring anchors for each rubric level.
- Require comments on any score at the minimum or maximum range.
- Flag variance when reviewer scores diverge by more than 2 points.

## Quality assurance checks
- 10% random sample audited by QA reviewer each round.
- 100% of awards above the standard threshold reviewed by QA.
- Reconcile eligibility flags before decisions are finalized.
- Validate narrative comments for clarity and respectful tone.

## Escalations and exceptions
- Escalate conflicts of interest to the review lead immediately.
- Route edge cases to a review triage huddle within 24 hours.
- Document exceptions with rationale, approver, and date.

## Metrics and reporting
- Median review time per application.
- Reviewer variance and calibration drift.
- Eligibility exception rate by cohort.
- QA re-score rate and root-cause categories.

## Tools and artifacts
- Rubric + scoring anchors (PDF + spreadsheet).
- Reviewer training deck and recorded session.
- QA checklist and escalation log.
- Award decision memo with final roster.

## Update cadence
- Post-cycle retro within 2 weeks of award decisions.
- Quarterly updates to rubric anchors and QA checklist.
- Annual audit of reviewer training materials.
